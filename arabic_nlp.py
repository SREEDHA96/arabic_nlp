# -*- coding: utf-8 -*-
"""arabic_nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hyoFQ7BYVlJ_gEvVQZHTGkQcRjf0YGyk
"""
# install necessary libraries
!pip install camel-tools faiss-cpu rank_bm25 streamlit --quiet

!pip install --force-reinstall numpy==1.24.4 --quiet

import os

folders = [
    "data/raw",
    "data/annotated",
    "scripts",
    "models/retriever",
    "dashboards"
]

for folder in folders:
    os.makedirs(folder, exist_ok=True)

print("Project folders created.")

import re

def normalize_arabic(text):
    """
    Normalize Arabic characters to reduce orthographic variation.

    This function performs:
    - Unification of different forms of alef to 'ا'
    - Replacement of 'ى' with 'ي'
    - Replacement of 'ؤ' with 'و'
    - Replacement of 'ئ' with 'ي'
    - Replacement of 'ة' with 'ه'
    - Collapse of multiple spaces into a single space

    Args:
        text (str): The Arabic text to normalize.

    Returns:
        str: The normalized Arabic text.
    """
    # Normalize different forms of alef (إأآا) to simple alef (ا)
    text = re.sub(r"[إأآا]", "ا", text)

    # Normalize final alef maqsura (ى) to yeh (ي)
    text = re.sub(r"ى", "ي", text)

    # Replace waw with hamza (ؤ) with simple waw (و)
    text = re.sub(r"ؤ", "و", text)

    # Replace yeh with hamza (ئ) with simple yeh (ي)
    text = re.sub(r"ئ", "ي", text)

    # Replace ta marbuta (ة) with heh (ه)
    text = re.sub(r"ة", "ه", text)

    # Remove extra whitespace
    text = re.sub(r"\s+", " ", text)

    return text.strip()


def remove_diacritics(text):
    """
    Remove Arabic diacritical marks (Tashkeel) from the text.

    Diacritics include: Fatha, Damma, Kasra, Sukun, Shadda, Tanween, etc.

    Args:
        text (str): The Arabic text to clean.

    Returns:
        str: Arabic text without diacritics.
    """
    # Arabic diacritics Unicode range:
    # \u0617–\u061A, \u064B–\u0652, \u0670, \u06D6–\u06ED
    arabic_diacritics = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED]")
    return re.sub(arabic_diacritics, '', text)


# Example usage
text = "إِنَّ اللَّهَ غَفُورٌ رَحِيمٌ"
print("Original:", text)
print("Cleaned :", normalize_arabic(remove_diacritics(text)))

### Create Simulated Annotated Data
# We’ll use a mock Arabic sentiment classification task where annotators label:

# POS: Positive

# NEG: Negative

# NEU: Neutral

import pandas as pd

# Sample Arabic sentences
texts = [
    "الخدمة ممتازة جدًا",           # POS
    "التجربة كانت سيئة",            # NEG
    "المكان عادي",                  # NEU
    "الطعام لذيذ ولكن بارد",         # MIX
    "الانتظار كان طويلًا",           # NEG
    "الموظفون ودودون",              # POS
    "لا يوجد موقف سيارات",           # NEG
    "كل شيء كان كما توقعت",          # NEU
    "لن أعود مرة أخرى",             # NEG
    "رائع جدًا ومميز"               # POS
]

# Simulated annotations by two annotators
annotator1 = ["POS", "NEG", "NEU", "POS", "NEG", "POS", "NEG", "NEU", "NEG", "POS"]
annotator2 = ["POS", "NEG", "NEU", "NEU", "NEG", "POS", "NEU", "NEU", "NEG", "POS"]

# Create DataFrame
df = pd.DataFrame({
    "Text": texts,
    "Annotator_1": annotator1,
    "Annotator_2": annotator2
})

df

from sklearn.metrics import cohen_kappa_score

# Encode labels as integers
label_map = {"POS": 1, "NEG": 0, "NEU": 2}
a1_encoded = [label_map[label] for label in df["Annotator_1"]]
a2_encoded = [label_map[label] for label in df["Annotator_2"]]

# Calculate Cohen's Kappa
kappa = cohen_kappa_score(a1_encoded, a2_encoded)

print(f"Cohen's Kappa Score: {kappa:.2f}")

# Sample knowledge base (corpus)
corpus = [
    "الطقس في مسقط مشمس اليوم.",
    "يوجد ازدحام مروري على الطريق السريع.",
    "وزارة الصحة تعلن عن حملة تطعيم جديدة.",
    "انخفضت أسعار النفط هذا الأسبوع.",
    "افتتاح مركز تسوق جديد في السيب.",
    "التعليم عن بعد سيستمر حتى نهاية الفصل.",
    "تم إطلاق خدمة الإنترنت الجديدة في عمان.",
    "يبدأ العمل بالتوقيت الشتوي الأسبوع القادم.",
    "تم اختيار عمان كأفضل وجهة سياحية.",
    "ارتفاع درجات الحرارة خلال عطلة نهاية الأسبوع."
]

# Simulated queries and ground truth (index of correct answer in corpus)
queries = [
    {"query": "ما حالة الطقس في مسقط؟", "answer_index": 0},
    {"query": "ما الأخبار عن حملة التطعيم؟", "answer_index": 2},
    {"query": "ما آخر أخبار أسعار النفط؟", "answer_index": 3},
    {"query": "متى يبدأ التوقيت الشتوي؟", "answer_index": 7},
    {"query": "هل هناك ازدحام مروري؟", "answer_index": 1}
]

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def retrieve_top_k(query, corpus, k=3):
    # Initialize TF-IDF vectorizer to convert text into numeric feature vectors
    vectorizer = TfidfVectorizer()

    # Fit the vectorizer on the corpus + query and transform them into a TF-IDF matrix
    # The query is appended at the end of the corpus list
    tfidf_matrix = vectorizer.fit_transform(corpus + [query])

    # Compute cosine similarity between the query vector (last row) and each document in the corpus
    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])

    # Get indices of top-k most similar documents in descending order of similarity
    top_k = cosine_sim.argsort()[0][-k:][::-1]

    # Return the indices of the top-k retrieved documents
    return top_k

def evaluate_retriever(queries, corpus, k=3):
    # Initialize the sum of reciprocal ranks for all queries
    total_reciprocal_rank = 0

    # List to keep track of precision@k (1 if relevant doc found in top-k, else 0) for each query
    precision_at_k_list = []

    # Loop through each query in the query set
    for q in queries:
        # Retrieve indices of top-k most relevant documents for the query
        top_k = retrieve_top_k(q["query"], corpus, k)

        # Ground truth index of the relevant document for the current query
        gt_index = q["answer_index"]

        # Calculate Mean Reciprocal Rank (MRR) component
        if gt_index in top_k:
            # Find the rank (1-based position) of the correct document in the retrieved list
            rank = top_k.tolist().index(gt_index) + 1

            # Add reciprocal of the rank to the total reciprocal rank sum
            total_reciprocal_rank += 1 / rank
        else:
            # If the relevant document not in top-k, add zero
            total_reciprocal_rank += 0

        # Calculate Precision@k for current query (1 if relevant doc found in top-k, else 0)
        precision_at_k_list.append(1 if gt_index in top_k else 0)

        # Print query information and evaluation results
        print(f"Query: {q['query']}")
        print(f"Retrieved: {[corpus[i] for i in top_k]}")  # Show retrieved documents
        print(f"Ground Truth: {corpus[gt_index]}")         # Show relevant document
        print(f"P@{k}: {precision_at_k_list[-1]}\n")       # Show precision@k for this query

    # Calculate average precision@k over all queries
    avg_precision = sum(precision_at_k_list) / len(queries)

    # Calculate mean reciprocal rank over all queries
    mrr = total_reciprocal_rank / len(queries)

    # Return both average precision@k and MRR
    return avg_precision, mrr

# Run evaluation with k=3 and print final aggregated metrics
p_at_3, mrr = evaluate_retriever(queries, corpus, k=3)
print(f"\nFinal Precision@3: {p_at_3:.2f}")
print(f"Final MRR: {mrr:.2f}")

# Sample factual QA pairs for hallucination evaluation
factual_qa = [
    {"question": "ما عاصمة عمان؟", "answer": "مسقط"},                # Expected correct answer: "مسقط"
    {"question": "ما هو عدد أيام الأسبوع؟", "answer": "7"},           # Expected: "7"
    {"question": "من هو مؤسس الدولة العمانية الحديثة؟", "answer": "سعود بن فيصل"},  # Expected: "سعود بن فيصل"
]

# Simulated model outputs corresponding to each question above
model_outputs = [
    "مسقط",             # Correct match
    "ثمانية",           # Incorrect (hallucination)
    "سعود بن فيصل",     # Correct match
]

# Function to test for hallucinations by comparing expected and actual outputs
def hallucination_test(qa_pairs, outputs):
    hallucinations = 0                    # Counter to track hallucinated answers
    total = len(qa_pairs)                # Total number of QA pairs to evaluate

    # Iterate through each QA pair and corresponding model output
    for i, qa in enumerate(qa_pairs):
        is_hallucinated = (qa["answer"] != outputs[i])  # Check if answer mismatches
        if is_hallucinated:
            hallucinations += 1  # Increment if hallucinated
            print(f"Hallucination detected for Q: {qa['question']}")
            print(f"Expected: {qa['answer']} | Got: {outputs[i]}\n")  # Show expected vs actual

    # Calculate and print the hallucination rate
    hallucination_rate = hallucinations / total
    print(f"Hallucination Rate: {hallucination_rate:.2%}")  # Display as percentage
    return hallucination_rate  # Return the metric for further use

# Run the hallucination test
hallucination_rate = hallucination_test(factual_qa, model_outputs)

# Regression Test Suite

MAX_HALLUCINATION_RATE = 50  # Acceptable max hallucination rate (e.g., 15%)

def regression_test_suite(qa_pairs, model_outputs, max_hallucination_rate):
    print("Running Regression Test Suite...")
    hallucination_rate = hallucination_test(qa_pairs, model_outputs)

    if hallucination_rate > max_hallucination_rate:
        raise AssertionError(
            f"❌ Regression failed: Hallucination rate {hallucination_rate:.2%} exceeds threshold of {max_hallucination_rate:.2%}"
        )
    else:
        print(f"✅ Regression passed: Hallucination rate {hallucination_rate:.2%} within acceptable limits.")

regression_test_suite(factual_qa, model_outputs, MAX_HALLUCINATION_RATE)
